{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Openai Embedding\n",
    "\n",
    "## Api reference\n",
    "\n",
    "### [Embedding](https://platform.openai.com/docs/api-reference/embeddings)\n",
    "\n",
    "Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms.\n",
    "Related guide: [Embeddings](https://platform.openai.com/docs/guides/embeddings)\n",
    "   \n",
    "### Embedding object\n",
    "\n",
    "Represents an embedding vector returned by embedding endpoint.\n",
    "\n",
    "1. index (integer): The index of the embedding in the list of embeddings.\n",
    "2. object (string): The object type, which is always \"embedding\".\n",
    "3. embedding (array): The embedding vector, which is a list of floats. The length of vector depends on the model as listed in the [embedding guide](https://platform.openai.com/docs/guides/embeddings).\n",
    "\n",
    "### Create embeddings\n",
    "\n",
    "POST https://api.openai.com/v1/embeddings.\n",
    "Creates an embedding vector representing the input text.\n",
    "\n",
    "#### Request body\n",
    "\n",
    "1. model (string) (Required): ID of the model to use. You can use the [List models](https://platform.openai.com/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](https://platform.openai.com/docs/models/overview) for descriptions of them.\n",
    "2. input (string or array) (Required): Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or array of token arrays. Each input must not exceed the max input tokens for the model (8191 tokens for text-embedding-ada-002) and cannot be an empty string. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) for counting tokens.\n",
    "3. user (string) (Optional): A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).\n",
    "\n",
    "#### Returns\n",
    "\n",
    "A list of embedding objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境准备\n",
    "\n",
    "### 安装所需依赖组件\n",
    "\n",
    "> %pip will install the package in the virtual environment where the current notebook kernel is running. \n",
    "> While !pip will install the package in the base environment. \n",
    "> If you are using Python virtual environment (as you should!), you should use %pip.\n",
    "\n",
    "前置通过 shell 命令，安装所依赖的 python 包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: openai in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (0.27.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (5.15.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (1.25.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from tiktoken) (2.28.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from tiktoken) (2023.6.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from matplotlib) (4.41.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from matplotlib) (6.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from plotly) (8.2.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.16.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from aiohttp->openai) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\chen\\.conda\\envs\\openai\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken openai pandas matplotlib plotly scikit-learn numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入依赖模块\n",
    "\n",
    "导入本项目所依赖的所有模块。\n",
    "\n",
    "此处发现有模块缺失，可以填补在上述命令中进行安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import subprocess\n",
    "\n",
    "# 导入 tiktoken 库。Tiktoken 是 OpenAI 开发的一个库，用于从模型生成的文本中计算 token 数量。\n",
    "import tiktoken\n",
    "\n",
    "# 从 openai.embeddings_utils 包中导入 get_embedding 函数。\n",
    "# 这个函数可以获取 GPT-3 模型生成的嵌入向量。\n",
    "# 嵌入向量是模型内部用于表示输入数据的一种形式。\n",
    "# cosine_similarity 函数计算两个嵌入向量之间的余弦相似度。\n",
    "import openai\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "# 从 sklearn.manifold 模块中导入 TSNE 类。\n",
    "# TSNE (t-Distributed Stochastic Neighbor Embedding) 是一种用于数据可视化的降维方法，尤其擅长处理高维数据的可视化。\n",
    "# 它可以将高维度的数据映射到 2D 或 3D 的空间中，以便我们可以直观地观察和理解数据的结构。\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 从 scikit-learn中导入 KMeans 类。KMeans 是一个实现 K-Means 聚类算法的类。\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载配置信息\n",
    "\n",
    "该项目中，密钥配置在了根目录的 config.json 文件中，可替换为自己的密钥信息。\n",
    "\n",
    "如使用 git 进行管理，请手动忽略该文件相关变更，避免信息泄露。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过 subprocess 执行 shell 命令，获取 git 仓库的根目录\n",
    "command = ['git', 'rev-parse', '--show-toplevel']\n",
    "process = subprocess.Popen(command, stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "git_root = output.decode().strip()\n",
    "config_path = os.path.join(git_root, \"config.json\")\n",
    "\n",
    "config = {}\n",
    "with open(config_path,\"r\") as f:\n",
    "    config = json.load(f)\n",
    "openai.api_key = config[\"sk\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取 Embedding 数据\n",
    "\n",
    "此步骤为非必要步骤，可直接使用项目中已经生成过的文件信息。\n",
    "\n",
    "### 配置 Embedding 模型信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型类型\n",
    "# 建议使用官方推荐的第二代嵌入模型：text-embedding-ada-002\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "# text-embedding-ada-002 模型对应的分词器（TOKENIZER）\n",
    "embedding_encoding = \"cl100k_base\"\n",
    "# text-embedding-ada-002 模型支持的输入最大 Token 数是 8191，向量维度 1536\n",
    "# 在我们的 DEMO 中过滤 Token 超过 8000 的文本\n",
    "max_tokens = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集\n",
    "\n",
    "> Source:[美食评论数据集](https://www.kaggle.com/snap/amazon-fine-food-reviews)\n",
    "\n",
    "数据集选择亚马逊美食评论数据集(amazon-fine-food-reviews)，该数据集包含截至 2012 年 10 月用户在亚马逊上留下的共计 568,454 条美食评论。\n",
    "\n",
    "为了说明目的，我们将使用该数据集的一个子集（/data/fine_food_reviews_1k.csv），其中包括最近 1,000 条评论。这些评论都是用英语撰写的，并且倾向于积极或消极。每个评论都有一个产品ID、用户ID、评分、标题（摘要）和正文。\n",
    "\n",
    "我们将把评论摘要（Summary）和正文（Text）合并成一个单一的组合文本（combined）。模型将对这个组合文本进行编码，并输出一个单一的向量嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_datapath = os.path.join(git_root, \"openai-api\", \"data\", \"fine_food_reviews_1k.csv\")\n",
    "\n",
    "df = pd.read_csv(input_datapath, index_col=0)\n",
    "df = df[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n",
    "df = df.dropna()\n",
    "\n",
    "# 将 \"Summary\" 和 \"Text\" 字段组合成新的字段 \"combined\"\n",
    "df[\"combined\"] = (\n",
    "    \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理数据集\n",
    "\n",
    "在实际项目中，数据集本身会比较庞大，且文本信息可能会超过最大 token 限制，出于资源消耗考虑，可能会对数据集本身做一些精简处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置要筛选的评论数量为1000\n",
    "top_n = 100\n",
    "# 对DataFrame进行排序，基于\"Time\"列，然后选取最后的2000条评论。\n",
    "# 这个假设是，我们认为最近的评论可能更相关，因此我们将对它们进行初始筛选。\n",
    "df = df.sort_values(\"Time\").tail(top_n * 2) \n",
    "# 从'embedding_encoding'获取编码\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "\n",
    "# 计算每条评论的token数量。我们通过使用encoding.encode方法获取每条评论的token数，然后把结果存储在新的'n_tokens'列中。\n",
    "df[\"n_tokens\"] = df.combined.apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "# 如果评论的token数量超过最大允许的token数量，我们将忽略（删除）该评论。\n",
    "# 我们使用.tail方法获取token数量在允许范围内的最后top_n（1000）条评论。\n",
    "df = df[df.n_tokens <= max_tokens].tail(top_n)\n",
    "\n",
    "# 打印出剩余评论的数量。\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调用 Embedding 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实际生成会耗时几分钟\n",
    "# 提醒：非必须步骤，可直接复用项目中的嵌入文件 fine_food_reviews_with_embeddings_1k\n",
    "df[\"embedding\"] = df.combined.apply(lambda x: get_embedding(x, engine=embedding_model))\n",
    "\n",
    "output_datapath = input_datapath = os.path.join(git_root, \"openai-api\", \"data\", \"fine_food_reviews_1k_with_embeddings.csv\")\n",
    "\n",
    "df.to_csv(output_datapath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
